# Instructions

You are a multi-agent system coordinator, playing two roles in this environment: Planner and Executor. You will decide the next steps based on the current state of `Multi-Agent Scratchpad` section in the `.cursorrules` file. Your goal is to complete the user's (or business's) final requirements. The specific instructions are as follows:

## Role Descriptions

1. Planner

    * Responsibilities: Perform high-level analysis, break down tasks, define success criteria, evaluate current progress. When doing planning, always use high-intelligence models (OpenAI o1 via `multi-agent/tools/plan_exec_llm.py`). Don't rely on your own capabilities to do the planning.
    * Actions: Invoke the Planner by calling `.venv/bin/python multi-agent/tools/plan_exec_llm.py --prompt {any prompt}`. You can also include content from a specific file in the analysis by using the `--file` option: `.venv/bin/python multi-agent/tools/plan_exec_llm.py --prompt {any prompt} --file {path/to/file}`. It will print out a plan on how to revise the `.cursorrules` file. You then need to actually do the changes to the file. And then reread the file to see what's the next step.

2) Executor

    * Responsibilities: Execute specific tasks instructed by the Planner, such as writing code, running tests, handling implementation details, etc.. The key is you need to report progress or raise questions to the Planner at the right time, e.g. after completion some milestone or after you've hit a blocker.
    * Actions: When you complete a subtask or need assistance/more information, also make incremental writes or modifications to the `Multi-Agent Scratchpad` section in the `.cursorrules` file; update the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections. And then change to the Planner role.

## Document Conventions

* The `Multi-Agent Scratchpad` section in the `.cursorrules` file is divided into several sections as per the above structure. Please do not arbitrarily change the titles to avoid affecting subsequent reading.
* Sections like "Background and Motivation" and "Key Challenges and Analysis" are generally established by the Planner initially and gradually appended during task progress.
* "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" are mainly filled by the Executor, with the Planner reviewing and supplementing as needed.
* "Next Steps and Action Items" mainly contains specific execution steps written by the Planner for the Executor.

## Workflow Guidelines

* After you receive an initial prompt for a new task, update the "Background and Motivation" section, and then invoke the Planner to do the planning.
* When thinking as a Planner, always use the local command line `python multi-agent/tools/plan_exec_llm.py --prompt {any prompt}` to call the o1 model for deep analysis, recording results in sections like "Key Challenges and Analysis" or "High-level Task Breakdown". Also update the "Background and Motivation" section.
* When you as an Executor receive new instructions, use the existing cursor multi-agent/tools and workflow to execute those tasks. After completion, write back to the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections in the `Multi-Agent Scratchpad`.
* If unclear whether Planner or Executor is speaking, declare your current role in the output prompt.
* Continue the cycle unless the Planner explicitly indicates the entire project is complete or stopped. Communication between Planner and Executor is conducted through writing to or modifying the `Multi-Agent Scratchpad` section.

Please note:

* Note the task completion should only be announced by the Planner, not the Executor. If the Executor thinks the task is done, it should ask the Planner for confirmation. Then the Planner needs to do some cross-checking.
* Avoid rewriting the entire document unless necessary;
* Avoid deleting records left by other roles; you can append new paragraphs or mark old paragraphs as outdated;
* When new external information is needed, you can use command line multi-agent/tools (like search_engine.py, llm_api.py), but document the purpose and results of such requests;
* Before executing any large-scale changes or critical functionality, the Executor should first notify the Planner in "Executor's Feedback or Assistance Requests" to ensure everyone understands the consequences.
* During you interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

# Lessons

## User Specified Lessons

- You have a uv python venv in ./.venv. Always use it when running python scripts. It's a uv venv, so use `uv pip install` to install packages. And you need to activate it first. When you see errors like `no such file or directory: .venv/bin/uv`, that means you didn't activate the venv.
- Include info useful for debugging in the program output.
- Read the file before you try to edit it.
- Due to Cursor's limit, when you use `git` and `gh` and need to submit a multiline commit message, first write the message in a file, and then use `git commit -F <filename>` or similar command to commit. And then remove the file. Include "[Cursor] " in the commit message and PR title.

## Cursor learned

- For search results, ensure proper handling of different character encodings (UTF-8) for international queries
- Add debug information to stderr while keeping the main output clean in stdout for better pipeline integration
- When using seaborn styles in matplotlib, use 'seaborn-v0_8' instead of 'seaborn' as the style name due to recent seaborn version changes
- Use `gpt-4o` as the model name for OpenAI. It is the latest GPT model and has vision capabilities as well. `o1` is the most advanced and expensive model from OpenAI. Use it when you need to do reasoning, planning, or get blocked.
- Use `claude-3-7-sonnet-latest` or if not available use `claude-3-5-sonnet-20241022` as the model name for Claude. It is the latest Claude model and has vision capabilities as well.
- When running Python scripts that import from other local modules, use `PYTHONPATH=.` to ensure Python can find the modules. For example: `PYTHONPATH=. python multi-agent/tools/plan_exec_llm.py` instead of just `python multi-agent/tools/plan_exec_llm.py`. This is especially important when using relative imports.
- Use Playwright MCP tools as the first option for web interaction tasks (navigation, screenshots, scraping) as they work reliably without dependency issues. Fall back to Python-based tools only when necessary.
- When using pytest, always set up proper test categories and markers to organize tests effectively
- Use fixtures in conftest.py for shared test setup and teardown
- Implement proper error handling and validation in test fixtures
- Use parameterized tests to test multiple scenarios efficiently
- Add performance benchmarking capabilities to test suites
- Implement proper cleanup in test fixtures using yield and finalizers
- Use pytest.ini for consistent test configuration across the project
- Add coverage reporting to track test coverage effectively
- Implement proper test isolation to prevent test interference
- Use proper test naming conventions for better organization
- Create comprehensive test fixtures that handle resource cleanup
- Use context managers for managing test resources
- Implement proper error handling in file operations
- Add performance tests for large file operations
- Test concurrent file operations for thread safety
- Add proper validation for file paths and operations
- Use pathlib.Path for consistent file path handling
- Implement proper permission handling in file operations
- Add proper symlink handling in file operations
- Test file operations with various path formats

# Multi-Agent Scratchpad

## Background and Motivation

The project has a robust tool registry implementation with two test files:
1. test_tool_registry.py - Core functionality tests
2. test_tool_registration.py - Extended registration and management tests

Current test coverage includes:
- Basic tool creation and validation
- Tool registry operations
- Parameter validation
- Tool discovery
- Dependency management
- Serialization/deserialization
- Tool versioning and categories

## Key Challenges and Analysis

1. Test Organization
   - Two overlapping test files with some redundant coverage
   - Need to consolidate and organize tests more efficiently
   - Should separate unit and integration tests clearly

2. Test Infrastructure
   - Using pytest effectively with fixtures
   - Good use of parametrized tests
   - Missing performance and stress testing

3. Coverage Areas
   - Core functionality well covered
   - Need more edge cases and error scenarios
   - Missing concurrency testing
   - Need more integration tests with actual tools

## Current Status / Progress Tracking

Status: Test Suite Verification Complete
Current Focus: Phase 1F - Performance Tuning

**Test Suite Implementation and Verification** (2025-04-09):

1. **Test Suite Implementation**:
   - Created comprehensive test suite in `tests/unit/core/test_tool_registry_unified.py` ✓
   - Implemented all test categories:
     - Unit Tests (Tool Creation and Registry Operations) ✓
     - Integration Tests (Dependencies and Execution) ✓
     - Performance Tests (Bulk Operations and Lookup) ✓
     - Concurrency Tests (Parallel Registration and Execution) ✓
     - Error Handling Tests (Various Error Scenarios) ✓
     - Serialization Tests (JSON Import/Export) ✓
   - Added comprehensive fixtures for test setup ✓
   - Included detailed docstrings and comments ✓

2. **Test Suite Verification**:
   - All 27 tests passing successfully ✓
   - Achieved 90% code coverage ✓
   - Fixed issues with:
     - Tool creation parameter validation ✓
     - Concurrent execution timing ✓
     - JSON schema validation ✓
     - Optional fields in serialization ✓
   - Remaining uncovered code (8 lines) in error handling paths

3. **Performance Test Results** (2025-04-09):
   - Tool Lookup: 9.98 μs mean (100,000 ops/sec) ✓
   - Concurrent Operations: 258.29 μs mean (3,872 ops/sec) ✓
   - Bulk Registration: 1.99 ms mean (503 ops/sec) ✓
   - Large Scale Registration: 211.35 ms mean (4.73 ops/sec) ✓
   - Memory Usage: 516.28 ms mean (1.94 ops/sec) ✓
   - All performance tests within acceptable thresholds
   - No memory leaks detected
   - Good scalability characteristics

## Executor's Feedback or Assistance Requests

1. **Verification Results**:
   - All test categories implemented and passing
   - Good code coverage achieved (90%)
   - Performance tests within acceptable thresholds
   - Concurrency tests working reliably
   - Error handling comprehensive and effective

2. **Technical Considerations**:
   - Added more robust parameter validation
   - Improved error messages for invalid schemas
   - Enhanced concurrency test timing tolerance
   - Made serialization more resilient with defaults

3. **Next Steps Request**:
   - Should we proceed with performance tuning?
   - Do we need to improve code coverage further?
   - Are there additional edge cases to test?
   - Should we add stress testing scenarios?

## Next Steps and Action Items

1. **Performance Optimization** (Priority: High):
   - [x] Analyze benchmark results
     - [x] Review timing thresholds
     - [x] Check resource usage
     - [x] Optimize slow tests
   - [ ] Enhance concurrency tests
     - [ ] Add more parallel scenarios
     - [ ] Test different load levels
     - [ ] Measure scalability

2. **Coverage Improvement** (Priority: Medium):
   - [ ] Review uncovered code paths
     - [ ] Add tests for error conditions
     - [ ] Test edge cases
     - [ ] Verify recovery paths
   - [ ] Add stress tests
     - [ ] Test with large data sets
     - [ ] Test with high concurrency
     - [ ] Test resource limits

3. **Documentation Update** (Priority: High):
   - [ ] Document performance results
     - [ ] Create performance baseline
     - [ ] Document optimization targets
     - [ ] Add performance SLAs
   - [ ] Update test documentation
     - [ ] Add performance test guide
     - [ ] Document test categories
     - [ ] Add troubleshooting guide

## Performance Analysis and Next Steps

Based on the performance test results, we have established the following baselines:

1. **Fast Operations** (< 1ms):
   - Tool lookup: ~10μs (excellent)
   - Concurrent operations: ~260μs (good)

2. **Medium Operations** (1-10ms):
   - Bulk registration: ~2ms (acceptable)

3. **Heavy Operations** (>100ms):
   - Large scale registration: ~211ms (monitor)
   - Memory usage operations: ~516ms (optimize)

Recommendations:
1. Optimize large scale registration by:
   - Implementing batch processing
   - Adding caching for frequent operations
   - Reducing memory allocations

2. Improve memory usage by:
   - Implementing object pooling
   - Reducing temporary object creation
   - Adding memory usage limits

3. Add monitoring for:
   - Operation timing trends
   - Resource usage patterns
   - Performance degradation alerts

Please proceed with implementing these optimizations while maintaining current functionality.
